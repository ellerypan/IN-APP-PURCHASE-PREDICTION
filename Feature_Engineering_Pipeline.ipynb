{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "ss_cols = ['session_id', 'start_timestamp', 'timezone',\n",
    "       'timezone_offset', 'previous_sessions_duration',\n",
    "       'user_created_timestamp', 'is_user_first_session', 'is_session',\n",
    "       'is_developer', 'is_wau', 'is_mau', 'country', 'region', 'city',\n",
    "       'latitude', 'longitude', 'locale', 'os_name', 'session_index',\n",
    "       'device_id', 'user_id_hash']\n",
    "\n",
    "att_value = pd.read_csv('att.csv',usecols=['attribute_value'])\n",
    "ses = pd.read_csv('sessions.csv',usecols=ss_cols)\n",
    "eve = pd.read_csv(\"events.csv\")\n",
    "att = pd.read_csv('att.csv',usecols=['session_id', 'attribute','user_id_hash'])\n",
    "mes = pd.read_csv('mes.csv')\n",
    "sub = pd.read_csv('sample_submission_2.csv')\n",
    "# label\n",
    "eve['purchase'] = eve[eve['event'] == '8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrows of att: 185590092\n",
      "nrows of eve: 111946597\n",
      "nrows of ses: 6239836\n",
      "nrows of mes: 2896\n",
      "ncols of att: 3\n",
      "ncols of eve: 11\n",
      "ncols of ses: 21\n",
      "ncols of mes: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"nrows of att:\",len(att_without_hash))\n",
    "print(\"nrows of eve:\",len(eve))\n",
    "print(\"nrows of ses:\",len(ses))\n",
    "print(\"nrows of mes:\",len(mes))\n",
    "print(\"ncols of att:\",len(att_without_hash.columns))\n",
    "print(\"ncols of eve:\",len(eve.columns))\n",
    "print(\"ncols of ses:\",len(ses.columns))\n",
    "print(\"ncols of mes:\",len(mes.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id         0\n",
       "event              0\n",
       "event_value        0\n",
       "event_month        0\n",
       "event_day          0\n",
       "event_dow          0\n",
       "event_hour         0\n",
       "purchase           0\n",
       "event_timestamp    0\n",
       "event_week         0\n",
       "user_id_le         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eve.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id    0\n",
       "attribute     0\n",
       "user_id_le    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "session_id                        0\n",
       "start_timestamp                   0\n",
       "timezone                       9538\n",
       "timezone_offset                9538\n",
       "previous_sessions_duration        0\n",
       "user_created_timestamp            0\n",
       "is_user_first_session             0\n",
       "is_session                        0\n",
       "is_developer                      0\n",
       "is_wau                            0\n",
       "is_mau                            0\n",
       "country                       16657\n",
       "region                        12926\n",
       "city                          12918\n",
       "latitude                      12918\n",
       "longitude                     12918\n",
       "locale                         9538\n",
       "os_name                       27444\n",
       "session_index                     0\n",
       "device_id                     18073\n",
       "user_id_hash                      0\n",
       "user_id_le                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ses.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "app_id                   0\n",
       "message_id               0\n",
       "action_type              0\n",
       "delivery_type            0\n",
       "delivery_time_mode       0\n",
       "goal_kind             1640\n",
       "dtype: int64"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mes.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode hash, lower size significantly\n",
    "from sklearn import preprocessing\n",
    "\n",
    "ur_hash = set(eve['user_id_hash'])|set(att['user_id_hash'])\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(np.array(list(ur_hash)))\n",
    "eve['user_id_le'] = le.transform(eve['user_id_hash'])\n",
    "att['user_id_le'] = le.transform(att['user_id_hash'])\n",
    "ses['user_id_le'] = le.transform(ses['user_id_hash'])\n",
    "sub['user_id_le'] = le.transform(sub['user_id_hash'])\n",
    "att.drop(['user_id_hash'],axis=1,inplace=True)\n",
    "eve.drop(['user_id_hash'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 185590092 entries, 0 to 185590091\n",
      "Data columns (total 3 columns):\n",
      "session_id    int64\n",
      "attribute     int64\n",
      "user_id_le    int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 4.1 GB\n"
     ]
    }
   ],
   "source": [
    "att.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 111946597 entries, 15817265 to 108027205\n",
      "Data columns (total 11 columns):\n",
      "session_id         int64\n",
      "event              object\n",
      "event_value        float64\n",
      "event_month        int64\n",
      "event_day          int64\n",
      "event_dow          int64\n",
      "event_hour         int64\n",
      "purchase           int64\n",
      "event_timestamp    datetime64[ns]\n",
      "event_week         int64\n",
      "user_id_le         int64\n",
      "dtypes: datetime64[ns](1), float64(1), int64(8), object(1)\n",
      "memory usage: 10.0+ GB\n"
     ]
    }
   ],
   "source": [
    "eve.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_cnt = Counter()\n",
    "for e in eve['event']:\n",
    "    try: \n",
    "        event_cnt[int(e)]+=1\n",
    "    except:\n",
    "        continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Attribute\n",
    "Compare those who have been purchased and who haven't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usr_att = att.groupby(['user_id_le'],as_index=False)['attribute']\\\n",
    ".agg({\"att_cnt\":\"count\",\"is67\":lambda x:int(67 in x),\"is96\":lambda x:int(96 in x)})\n",
    "# purchased\n",
    "user_pch = pd.DataFrame({\"user_id_le\":purchases.user_id_le.unique()}).sort_values(by='user_id_le')\n",
    "user_pch = pd.merge(user_pch,usr_att,on=['user_id_le'],how='left')\n",
    "user_pch = pd.merge(user_pch,usr_att_val,on=['user_id_le'],how='left')\n",
    "user_pch = pd.merge(user_pch,usr_att_val2,on=['user_id_le'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-purchsed\n",
    "nonpch = pd.DataFrame({\"user_id_le\":list(set(eve.user_id_le)-set(user_pch.user_id_le))})\n",
    "nonpch = pd.merge(nonpch,usr_att,on=['user_id_le'],how='left')\n",
    "nonpch = pd.merge(nonpch,usr_att_val,on=['user_id_le'],how='left')\n",
    "nonpch = pd.merge(nonpch,usr_att_val2,on=['user_id_le'],how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attribute_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_value['length'] = att_value['attribute_value'].apply(lambda x:len(str(x)))\n",
    "att_value = pd.DataFrame({\"user_id_le\":att['user_id_le'],\"attribute_value\":att_value['attribute_value'],'att_val_len':att_value['length']})\n",
    "\n",
    "usr_att_val = att_value.groupby(['user_id_le'],as_index=False)['attribute_value']\\\n",
    ".agg({\"att_val_cnt\":\"count\",\"att_val_uniq\":\"nunique\"})\n",
    "usr_att_val2 = att_value.groupby(['user_id_le'],as_index=False)['att_val_len']\\\n",
    ".agg({\"att_len_mean\":\"mean\",\"att_len_max\":\"max\",\"att_len_min\":\"min\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go further on this feature. U can totally skip this chunk since I save all important info in `user_att.csv` and eventually combine those info into training and testing dataset.\n",
    "\n",
    "Basic idea:\n",
    "- explore features around **attribute value** on **7 and 14 days**\n",
    "- create same features for training set with **last** 7 and 14 days, otherwise there would be an infomation leak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **buy_mean** equals to purchasing count in certain period / occuring times in `attribute.csv`. You can imagine the more this value is, the more likely user would go on purchasing recently.\n",
    " \n",
    " - **bracket**  is tricky feature. During EDA I found some users whose *attribute_value* was encoded with **[]** seldom purchase. Later feature importance of tree models proved this thought.\n",
    " \n",
    " - **length** is about the length of *attribute_value*, it's useful too due to some values encoded into long and mysterious terms, that might imply sth.\n",
    "\n",
    "- **att_ses_ct** is a feature extracted from `attribute.csv` merged with `session.csv`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "eve['event_timestamp'] = eve['event_timestamp'].apply(lambda x:datetime.fromtimestamp(x/1000))\n",
    "eve['event_month'] = eve['event_timestamp'].apply(lambda x:x.month)\n",
    "eve['event_day'] = eve['event_timestamp'].apply(lambda x:x.day)\n",
    "eve['event_dow'] = eve['event_timestamp'].apply(lambda x:x.dayofweek)\n",
    "eve['event_hour'] = eve['event_timestamp'].apply(lambda x:x.hour)\n",
    "# fillna\n",
    "eve['event_value'] = eve['event_value'].fillna(0)\n",
    "# delete useless cols\n",
    "eve.drop(['event_timestamp'],inplace=True,axis=1)\n",
    "eve.drop(['app_id'],axis=1,inplace=True)\n",
    "eve = eve.sort_values(by=['user_id_le','event_timestamp'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses['start_timestamp'] = ses['start_timestamp'].apply(lambda x:datetime.fromtimestamp(x/1000))\n",
    "ses['user_created_timestamp'] = ses['user_created_timestamp'].apply(lambda x:datetime.fromtimestamp(x/1000))\n",
    "ses['duration_since_start'] = ses['start_timestamp']-ses['user_created_timestamp']\n",
    "ses_col = [c for c in ses.columns if c not in ['timezone', 'timezone_offset','is_developer','device_id','region', 'city', 'latitude', 'longitude','locale','user_id_hash']]\n",
    "ses_c = ses[ses_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranform time durations into hours\n",
    "ses['duration_since_start'] = ses['duration_since_start'].apply(lambda x:round(x/(np.timedelta64(1,'h')*24),3))\n",
    "ses['previous_sessions_duration'] =ses['previous_sessions_duration'].apply(lambda x:round(x/(3600*24),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mis_impute(data):\n",
    "    for i in data.columns:\n",
    "        if data[i].dtype == \"object\":\n",
    "            data[i] = data[i].fillna(\"other\")\n",
    "        elif (data[i].dtype == \"int64\" or data[i].dtype == \"float64\"):\n",
    "            data[i] = data[i].fillna(data[i].mean())\n",
    "        else:\n",
    "            pass\n",
    "    return data\n",
    "ses = mis_impute(ses)\n",
    "\n",
    "ses['ses_month'] = ses['start_timestamp'].apply(lambda x:x.month)\n",
    "# ses['ses_week'] = ses['start_timestamp'].apply(lambda x:x.week)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week tags\n",
    "we need to create week tags manually in this case since a week starts from Saturday to Friday next week here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_dt = pd.date_range('2018-09-29', freq='7D', periods=12)\n",
    "bins_str = bins_dt.astype(str).values\n",
    "labels = [i for i in range(0,len(bins_str)-1)] # 0 = 9.29-10.5,...,9=12.1-12.7,10=12.8-12.14\n",
    "\n",
    "eve['event_week'] = pd.cut(eve.event_timestamp.astype(np.int64)//10**9,\n",
    "                   bins=bins_dt.astype(np.int64)//10**9,\n",
    "                   labels=labels)\n",
    "\n",
    "ses['ses_week'] = pd.cut(ses.start_timestamp.astype(np.int64)//10**9,\n",
    "                   bins=bins_dt.astype(np.int64)//10**9,\n",
    "                   labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Stage1 -FE on `event.csv` and `session.csv`: most of them created around time duration.\n",
    "\n",
    "\n",
    "- Stage2 - Most features from `attribute.csv`: add our above foundings from attribute data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract features\n",
    "def get_time_gap(strs,parm):\n",
    "    time = strs.split(\":\")\n",
    "    time = list(set(time))\n",
    "    time = sorted(list(map(lambda x:int(x),time)))\n",
    "    time_gap = []\n",
    "    # active on that day\n",
    "    if len(time) == 1:\n",
    "        return -20\n",
    "\n",
    "    for index, value in enumerate(time):\n",
    "        if index <= len(time) - 2:\n",
    "            gap = abs(time[index] - time[index + 1])\n",
    "            time_gap.append(gap)\n",
    "\n",
    "    if parm == '1':\n",
    "        return np.mean(time_gap)\n",
    "    elif parm == '2':\n",
    "        return np.max(time_gap)\n",
    "    elif parm == '3':\n",
    "        return np.min(time_gap)\n",
    "    elif parm == '4':\n",
    "        return np.std(time_gap)\n",
    "    elif parm == '5':\n",
    "        return sp.stats.skew(time_gap)\n",
    "    elif parm == '6':\n",
    "        return sp.stats.kurtosis(time_gap)\n",
    "\n",
    "def get_day_repeat_count(strs):\n",
    "    time = strs.split(\":\")\n",
    "    time = dict(Counter(time))\n",
    "    time = sorted(time.items(), key=lambda x: x[1], reverse=False)\n",
    "    # show up once in one day\n",
    "    if (len(time) == 1) & (time[0][1] == 1):\n",
    "        return 0\n",
    "    # show up many times in one day\n",
    "    elif (len(time) == 1) & (time[0][1] > 1):\n",
    "        return 1\n",
    "    # show up many times in multiple days\n",
    "    elif (len(time) > 1) & (time[0][1] >= 2):\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "    \n",
    "def get_continue_launch_count(strs,parm):\n",
    "    time = strs.split(\":\")\n",
    "    time = dict(Counter(time))\n",
    "    time = sorted(time.items(), key=lambda x: x[0], reverse=False)\n",
    "    key_list = []\n",
    "    value_list = []\n",
    "    if len(time) == 1:\n",
    "        return -2\n",
    "    for key,value in dict(time).items():\n",
    "        key_list.append(int(key))\n",
    "        value_list.append(int(value))\n",
    "\n",
    "    if np.mean(np.diff(key_list, 1)) == 1:\n",
    "        if parm == '1':\n",
    "            return np.mean(value_list)\n",
    "        elif parm == '2':\n",
    "            return np.max(value_list)\n",
    "        elif parm == '3':\n",
    "            return np.min(value_list)\n",
    "        elif parm == '4':\n",
    "            return np.sum(value_list)\n",
    "        elif parm == '5':\n",
    "            return np.std(value_list)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def get_contin_day(day_list):\n",
    "    time = day_list.split(\":\")\n",
    "    time = list(map(lambda x:int(x),time))\n",
    "    m = np.array(time)\n",
    "    if len(set(m)) == 1:\n",
    "        return -1\n",
    "    m = list(set(m))\n",
    "    if len(m) == 0:\n",
    "        return -20\n",
    "    n = np.where(np.diff(m) == 1)[0]\n",
    "    i = 0\n",
    "    result = []\n",
    "    while i < len(n) - 1:\n",
    "        state = 1\n",
    "        while n[i + 1] - n[i] == 1:\n",
    "            state += 1\n",
    "            i += 1\n",
    "            if i == len(n) - 1:\n",
    "                break\n",
    "        if state == 1:\n",
    "            i += 1\n",
    "            result.append(2)\n",
    "        else:\n",
    "            i += 1\n",
    "            result.append(state + 1)\n",
    "    if len(n) == 1:\n",
    "        result.append(2)\n",
    "    if len(result) != 0:\n",
    "        return np.max(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fea_Eng(event,session,tag,add_label=False): \n",
    "    \n",
    "    # event part\n",
    "    eve_f1 = event.groupby(['user_id_le'],as_index=False)['event_timestamp'].agg({\"user_count_{}\".format(tag):\"count\"})\n",
    "    eve_f2 = event.groupby(['user_id_le'],as_index=False)['event_day'].agg({\"user_eve_day_count_{}\".format(tag): \"nunique\"})\n",
    "\n",
    "    eve_f3 = event[['user_id_le', 'event_day']]\n",
    "    eve_f3['event_day'] = eve_f3['event_day'].astype('str')\n",
    "\n",
    "    eve_f3 = eve_f3.groupby(['user_id_le'])['event_day'].agg(lambda x: ':'.join(x)).reset_index()\n",
    "    eve_f3.rename(columns={'event_day': 'act_list'}, inplace=True)\n",
    "\n",
    "    # time gap of the event per user\n",
    "    eve_f3['time_gap_mean_{}'.format(tag)] = eve_f3['act_list'].apply(get_time_gap,args=('1'))\n",
    "    eve_f3['time_gap_max_{}'.format(tag)] = eve_f3['act_list'].apply(get_time_gap,args=('2'))\n",
    "    eve_f3['time_gap_min_{}'.format(tag)] = eve_f3['act_list'].apply(get_time_gap,args=('3'))\n",
    "    eve_f3['time_gap_std_{}'.format(tag)] = eve_f3['act_list'].apply(get_time_gap,args=('4'))\n",
    "\n",
    "    # active count mean\n",
    "    eve_f3['mean_act_count_{}'.format(tag)] = eve_f3['act_list'].apply(lambda x: len(x.split(\":\")) / len(set(x.split(\":\"))))\n",
    "    # active date mean\n",
    "    eve_f3['act_mean_date_{}'.format(tag)] = eve_f3['act_list'].apply(lambda x: np.sum([int(ele) for ele in x.split(\":\")]) / len(x.split(\":\")))\n",
    "    # users show up many times in multiple days?\n",
    "    eve_f3['day_repeat_count_{}'.format(tag)] = eve_f3['act_list'].apply(get_day_repeat_count)\n",
    "    # Launch continuously?\n",
    "    eve_f3['con_act_day_count_mean_{}'.format(tag)] = eve_f3['act_list'].apply(get_continue_launch_count, args=('1'))\n",
    "    eve_f3['con_act_day_count_max_{}'.format(tag)] = eve_f3['act_list'].apply(get_continue_launch_count, args=('2'))\n",
    "    eve_f3['con_act_day_count_min_{}'.format(tag)] = eve_f3['act_list'].apply(get_continue_launch_count, args=('3'))\n",
    "    eve_f3['con_act_day_count_total_{}'.format(tag)] = eve_f3['act_list'].apply(get_continue_launch_count, args=('4'))\n",
    "    eve_f3['con_act_day_count_std_{}'.format(tag)] = eve_f3['act_list'].apply(get_continue_launch_count, args=('5'))\n",
    "    eve_f3['con_act_max_{}'.format(tag)] = eve_f3['act_list'].apply(get_contin_day)\n",
    "    del eve_f3['act_list']\n",
    "\n",
    "    # when is the next event happen?\n",
    "    eve_f4 = event[['user_id_le', 'event_timestamp']].sort_values(['user_id_le', 'event_timestamp'],ascending=False)\n",
    "    eve_f4['next_time'] = eve_f4.groupby(['user_id_le'])['event_timestamp'].diff(1).apply(np.abs)\n",
    "    eve_f4['next_time'] = eve_f4['next_time'].fillna(-1)\n",
    "    eve_f4 = eve_f4.groupby(['user_id_le'], as_index=False)['next_time'].agg({\n",
    "        'next_time_max': np.max,\n",
    "        'next_time_mean': np.nanmean,\n",
    "    })\n",
    "    # convert it into hour\n",
    "    eve_f4['next_time_max_{}'.format(tag)] = eve_f4['next_time_max'].apply(lambda x:round(x/np.timedelta64(1,'h'),3))\n",
    "    eve_f4['next_time_mean_{}'.format(tag)] = eve_f4['next_time_mean'].apply(lambda x:round(x/np.timedelta64(1,'h'),3))                                                     \n",
    "    eve_f4.drop(['next_time_max','next_time_mean'],axis=1,inplace=True)\n",
    "    \n",
    "    # purchase cnt\n",
    "    eve_f5 = event.groupby(['user_id_le'], as_index=False)['purchase'].agg({'user_pch_ct_{}'.format(tag): \"sum\"})\n",
    "    \n",
    "    # session part\n",
    "    ses_f1 = session.groupby(['user_id_le'],as_index=False)['is_session'].agg({\"avg_True_ses_{}\".format(tag):\"mean\"})\n",
    "    # how many cities correspond to one user id?\n",
    "    ses_f2 = session.groupby(['user_id_le'],as_index=False)['city'].agg({\"city_ct_{}\".format(tag):\"count\"})\n",
    "    # duration since start\n",
    "    ses_f3 = session.groupby(['user_id_le'],as_index=False)['duration_since_start'].agg({\"du_since_start_{}\".format(tag):\"max\"})\n",
    "    # duration of previous session\n",
    "    ses_f4 = session.groupby(['user_id_le'],as_index=False)['previous_sessions_duration'].agg({\"prev_ses_du_mean_{}\".format(tag):\"mean\"})\n",
    "    \n",
    "    # load target table\n",
    "    if add_label:\n",
    "        data = pd.read_csv('targets.csv',usecols=['user_id_hash','user_purchase_binary_7_days','user_purchase_binary_14_days'])\n",
    "        data['user_id_le'] = le.transform(data['user_id_hash'])\n",
    "        data = data[[ 'user_id_le', 'user_purchase_binary_7_days','user_purchase_binary_14_days']]\n",
    "    else:\n",
    "        data = pd.read_csv('targets.csv',usecols=['user_id_hash'])\n",
    "        data['user_id_le'] = le.transform(data['user_id_hash'])\n",
    "        data = data[['user_id_le']]\n",
    "    # eve\n",
    "    data = pd.merge(data, eve_f1, on=['user_id_le'], how='left')\n",
    "    data = pd.merge(data, eve_f2, on=['user_id_le'], how='left')\n",
    "    data = pd.merge(data, eve_f3, on=['user_id_le'], how='left')\n",
    "    data = pd.merge(data, eve_f4, on=['user_id_le'], how='left')\n",
    "    data = pd.merge(data, eve_f5, on=['user_id_le'], how='left')\n",
    "    # ses\n",
    "    data = pd.merge(data, ses_f1, on=['user_id_le'], how='left')\n",
    "    data = pd.merge(data, ses_f2, on=['user_id_le'], how='left')\n",
    "    data = pd.merge(data, ses_f3, on=['user_id_le'], how='left')\n",
    "    data = pd.merge(data, ses_f4, on=['user_id_le'], how='left')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(event,data,tag): # event is the recent month, recent two month\n",
    "    \n",
    "    # add f1 purchase time gap\n",
    "    eve_f6 = event[['user_id_le', 'event_timestamp']].sort_values(['user_id_le', 'event_timestamp'],ascending=False)\n",
    "    eve_f6['next_time'] = event[event['event'] == '8'].groupby(['user_id_le'])['event_timestamp'].diff(1).apply(np.abs)\n",
    "    eve_f6['next_time'] = eve_f6['next_time'].fillna(-1)\n",
    "    eve_f6 = eve_f6.groupby(['user_id_le'], as_index=False)['next_time'].agg({\n",
    "        'next_pur_time_max': np.max,\n",
    "        'next_pur_time_mean': np.nanmean,\n",
    "        'next_pur_time_min':np.min\n",
    "    })\n",
    "    # convert it into hour\n",
    "    eve_f6['next_pch_time_max_{}'.format(tag)] = eve_f6['next_pur_time_max'].apply(lambda x:round(x/np.timedelta64(1,'h'),3))\n",
    "    eve_f6['next_pch_time_mean_{}'.format(tag)] = eve_f6['next_pur_time_mean'].apply(lambda x:round(x/np.timedelta64(1,'h'),3))                                                     \n",
    "    eve_f6['next_pch_time_min_{}'.format(tag)] = eve_f6['next_pur_time_min'].apply(lambda x:round(x/np.timedelta64(1,'h'),3))                                                     \n",
    "    eve_f6.drop(['next_pur_time_max','next_pur_time_mean','next_pur_time_min'],axis=1,inplace=True)\n",
    "    \n",
    "    # add f2 purchase day gap\n",
    "    eve_f7 = event[event['event'] == '8'][['user_id_le', 'event_day']]\n",
    "    eve_f7['event_day'] = eve_f7['event_day'].astype('str')\n",
    "\n",
    "    eve_f7 = eve_f7.groupby(['user_id_le'])['event_day'].agg(lambda x: ':'.join(x)).reset_index()\n",
    "    eve_f7.rename(columns={'event_day': 'pch_list'}, inplace=True)\n",
    "\n",
    "    # time gap of the event per user\n",
    "    eve_f7['day_pch_gap_mean_{}'.format(tag)] = eve_f7['pch_list'].apply(get_time_gap,args=('1'))\n",
    "    eve_f7['day_pch_gap_max_{}'.format(tag)] = eve_f7['pch_list'].apply(get_time_gap,args=('2'))\n",
    "    eve_f7['day_pch_gap_min_{}'.format(tag)] = eve_f7['pch_list'].apply(get_time_gap,args=('3'))\n",
    "    eve_f7['day_pch_gap_std_{}'.format(tag)] = eve_f7['pch_list'].apply(get_time_gap,args=('4'))\n",
    "\n",
    "    # pch count mean\n",
    "    eve_f7['pch_mean_count_{}'.format(tag)] = eve_f7['pch_list'].apply(lambda x: len(x.split(\":\")) / len(set(x.split(\":\"))))\n",
    "    # pch date mean\n",
    "    eve_f7['pch_mean_date_{}'.format(tag)] = eve_f7['pch_list'].apply(lambda x: np.sum([int(ele) for ele in x.split(\":\")]) / len(x.split(\":\")))\n",
    "    # users show up many times in multiple days?\n",
    "    eve_f7['pch_day_repeat_count_{}'.format(tag)] = eve_f7['pch_list'].apply(get_day_repeat_count)\n",
    "    \n",
    "    # pch continuously?\n",
    "    eve_f7['con_pch_day_count_mean_{}'.format(tag)] = eve_f7['pch_list'].apply(get_continue_launch_count, args=('1'))\n",
    "    eve_f7['con_pch_day_count_max_{}'.format(tag)] = eve_f7['pch_list'].apply(get_continue_launch_count, args=('2'))\n",
    "    eve_f7['con_pch_day_count_min_{}'.format(tag)] = eve_f7['pch_list'].apply(get_continue_launch_count, args=('3'))\n",
    "    eve_f7['con_pch_day_count_total_{}'.format(tag)] = eve_f7['pch_list'].apply(get_continue_launch_count, args=('4'))\n",
    "    eve_f7['con_pch_day_count_std_{}'.format(tag)] = eve_f7['pch_list'].apply(get_continue_launch_count, args=('5'))\n",
    "    eve_f7['con_pch_max_{}'.format(tag)] = eve_f7['pch_list'].apply(get_contin_day)\n",
    "    del eve_f7['pch_list']\n",
    "    \n",
    "    \n",
    "    data = pd.merge(data,eve_f6,on=['user_id_le'],how='left')\n",
    "    data = pd.merge(data,eve_f7,on=['user_id_le'],how='left')\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attribute features merging\n",
    "def add_att(data):\n",
    "    data = pd.merge(data,usr_att,on=['user_id_le'],how='left')\n",
    "    data = pd.merge(data,usr_att_val,on=['user_id_le'],how='left')\n",
    "    data = pd.merge(data,usr_att_val2,on=['user_id_le'],how='left')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our training plan \n",
    "(number here means week tag)\n",
    "\n",
    "\n",
    "**7 days:**\n",
    "\n",
    "- $Fea_{0-8}$ + $Fea_{7-8}$ + $Fea_{8}$ $->$ $Purchase_{9}$ (train-optional)\n",
    "- $Fea_{0-9}$ + $Fea_{8-9}$ + $Fea_{9}$ $->$ $Purchase_{10}$ (train-**used**)\n",
    "- $Fea_{0-10}$ + $Fea_{9-10}$ + $Fea_{10}$ $->$ $Purchase_{11}$ (predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2018-12-08 00:00:00.922000')"
      ]
     },
     "execution_count": 1535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "eve[eve['event_week']==9]['event_timestamp'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific Dataset used for creating features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 days for training\n",
    "eve09 = eve[eve['event_week']<=9]\n",
    "eve89 = eve[(eve['event_week']<=9)&(eve['event_week']>=8)]\n",
    "eve9 = eve[eve['event_week']==9]\n",
    "\n",
    "eve69 = eve[(eve['event_week']<=9)&(eve['event_week']>=6)]\n",
    "eve710 = eve[(eve['event_week']<=10)&(eve['event_week']>=7)]\n",
    "\n",
    "ses09 = ses[ses['ses_week']<=9]\n",
    "ses89 = ses[(ses['ses_week']<=9)&(ses['ses_week']>=8)]\n",
    "ses9 = ses[ses['ses_week']==9]\n",
    "\n",
    "df09 = Fea_Eng(eve09,ses09,tag='09',add_label=False)\n",
    "df89 = Fea_Eng(eve89,ses89,tag='89',add_label=False)\n",
    "df9 = Fea_Eng(eve9,ses9,tag='9',add_label=True)\n",
    "\n",
    "df_ = pd.merge(df09, df89, on=['user_id_le'], how='left')\n",
    "df = pd.merge(df_, df9, on=['user_id_le'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1056,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 days for prediction\n",
    "eve010 = eve[(eve['event_week']<=10)]\n",
    "eve910 = eve[(eve['event_week']<=10)&(eve['event_week']>=9)]\n",
    "eve10 = eve[eve['event_week']==10]\n",
    "\n",
    "ses010 = ses[(eve['event_week']<=10)]\n",
    "ses910 = ses[(ses['ses_week']<=10)&(ses['ses_week']>=9)]\n",
    "ses10 = ses[ses['ses_week']==10]\n",
    "\n",
    "df010 = Fea_Eng(eve010,ses010,tag='010',add_label=False)\n",
    "df910 = Fea_Eng(eve910,ses910,tag='910',add_label=False)\n",
    "df10 = Fea_Eng(eve10,ses10,tag='10',add_label=True)\n",
    "\n",
    "df_p_ = pd.merge(df010, df910, on=['user_id_le'], how='left')\n",
    "df_p = pd.merge(df_p_, df10, on=['user_id_le'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more features\n",
    "df = add_features(eve69,df,'rec_mon')\n",
    "df = add_att(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = add_features(eve710,df_p,'rec_mon')\n",
    "df_p = add_att(df_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14 days:**\n",
    "\n",
    "- $Fea_{0-7}$ + $Fea_{6-7}$ + $Fea_{7}$ $->$ $Purchase_{8-9}$ (train-optional)\n",
    "- $Fea_{0-8}$ + $Fea_{7-8}$ + $Fea_{8}$ $->$ $Purchase_{9-10}$ (train-**used**)\n",
    "- $Fea_{0-10}$ + $Fea_{9-10}$ + $Fea_{10}$ $->$ $Purchase_{11-12}$ (predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1057,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14 days for training\n",
    "eve08 = eve[eve['event_week']<=8]\n",
    "eve78 = eve[(eve['event_week']<=8)&(eve['event_week']>=7)]\n",
    "eve8 = eve[eve['event_week']==8]\n",
    "# add\n",
    "eve58 = eve[(eve['event_week']<=8)&(eve['event_week']>=5)]\n",
    "\n",
    "ses08 = ses[eve['event_week']<=8]\n",
    "ses78 = ses[(ses['ses_week']<=8)&(ses['ses_week']>=7)]\n",
    "ses8 = ses[ses['ses_week']==8]\n",
    "\n",
    "df08 = Fea_Eng(eve08,ses08,tag='08',add_label=False)\n",
    "df78 = Fea_Eng(eve78,ses78,tag='78',add_label=False)\n",
    "df8 = Fea_Eng(eve8,ses8,tag='8',add_label=True)\n",
    "\n",
    "df_2w_ = pd.merge(df08, df78, on=['user_id_le'], how='left')\n",
    "df_2w = pd.merge(df_2w_, df8, on=['user_id_le'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more features\n",
    "df_2w = add_features(eve58,df_2w,'rec_mon')\n",
    "df_2w = add_att(df_2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I don't need to create `df_2w_p` due to it's same as `df_p`, but it would be different eventually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add features about attr_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to split user_att table and apply those in prediction and training data\n",
    "\n",
    "# to df-->df7\n",
    "attv_f_tr7 = user_att[['user_id_le','att_ses_ct', '7bracket_tr', '7buy_mean_tr', '7buy_mean_tr>.8_tr',\n",
    "       '7length<3_tr']]\n",
    "# to df_2w-->df14\n",
    "attv_f_tr14 = user_att[['user_id_le','att_ses_ct', '14buy_mean_tr', '14bracket_tr', '14buy_mean_tr>.8_tr',\n",
    "       '14length<3_tr']]\n",
    "# to df_p-->df7_p\n",
    "attv_f_pd7 = user_att[['user_id_le', 'att_ses_ct','7bracket_p', '7buy_mean_p', '7buy_mean>.8_p',\n",
    "       '7length<3_p']]\n",
    "# to df_p-->df14_p\n",
    "attv_f_pd14 = user_att[['user_id_le', 'att_ses_ct','14buy_mean_p', '14bracket_p', '14buy_mean>.8_p',\n",
    "       '14length<3_p']]\n",
    "\n",
    "df7 = pd.merge(df,attv_f_tr7,on=['user_id_le'],how='left')\n",
    "df14 = pd.merge(df_2w,attv_f_tr14,on=['user_id_le'],how='left')\n",
    "df7_p = pd.merge(df_p,attv_f_pd7,on=['user_id_le'],how='left')\n",
    "df14_p = pd.merge(df_p,attv_f_pd14,on=['user_id_le'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1626,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many nans we have because time shifting is nearly unlikely to cover all userids.\n",
    "# put -1 because light gradient boost machine knows -1 is nan\n",
    "def impute_transfer_save(df,tag):\n",
    "    for c in df.columns:\n",
    "        df[c] = df[c].fillna(-1)\n",
    "    df['user_id_hash'] = le.inverse_transform(df['user_id_le'])\n",
    "    df.to_csv('{}.csv'.format(tag),index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1539,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out only the rows where they've purchased\n",
    "purchases = eve[eve['event'] == '8']\n",
    "dec_1 = datetime(2018, 12, 1)\n",
    "dec_7 = datetime(2018, 12, 8)\n",
    "\n",
    "seven_day_purchases = set(purchases[purchases['event_timestamp'] > dec_7].user_id_le)\n",
    "fourteen_day_purchases = set(purchases[purchases['event_timestamp'] > dec_1].user_id_le)\n",
    "\n",
    "targets = pd.DataFrame({'user_id_le': eve['user_id_le'].unique()})\n",
    "targets['user_purchase_binary_7_days'] = targets['user_id_le'].apply(\n",
    "    lambda x: 1 if x in seven_day_purchases else 0)\n",
    "targets['user_purchase_binary_14_days'] = targets['user_id_le'].apply(\n",
    "    lambda x: 1 if x in fourteen_day_purchases else 0)\n",
    "\n",
    "# get previous target for FeaEng\n",
    "# purchases = eve[eve['event'] == '8']\n",
    "# dec_1 = datetime(2018, 11, 24)\n",
    "# dec_7 = datetime(2018, 12, 1)\n",
    "# dec_15 = datetime(2018, 12, 8)\n",
    "\n",
    "# seven_day_purchases = set(purchases[(purchases['event_timestamp'] > dec_7)&(purchases['event_timestamp'] < dec_15)].user_id_le)\n",
    "# fourteen_day_purchases = set(purchases[(purchases['event_timestamp'] > dec_1)&(purchases['event_timestamp'] < dec_15)].user_id_le)\n",
    "\n",
    "# targets2 = pd.DataFrame({'user_id_le': eve['user_id_le'].unique()})\n",
    "# targets2['user_purchase_binary_7_days'] = targets2['user_id_le'].apply(\n",
    "#     lambda x: 1 if x in seven_day_purchases else 0)\n",
    "# targets2['user_purchase_binary_14_days'] = targets2['user_id_le'].apply(\n",
    "#     lambda x: 1 if x in fourteen_day_purchases else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4729"
      ]
     },
     "execution_count": 1540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "targets2['user_purchase_binary_7_days'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9251"
      ]
     },
     "execution_count": 1541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets2['user_purchase_binary_14_days'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1475,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['user_purchase_binary_7_days'],axis=1,inplace=True)\n",
    "df_2w.drop(['user_purchase_binary_14_days'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1476,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df,targets[['user_id_le','user_purchase_binary_7_days']],on=['user_id_le'],how='left')\n",
    "df_2w = pd.merge(df_2w,targets[['user_id_le','user_purchase_binary_14_days']],on=['user_id_le'],how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1477,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(df,tag):\n",
    "    df.to_csv('{}.csv'.format(tag),index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(df7,'df7_feav3')\n",
    "save(df14,'df14_feav3')\n",
    "save(df7_p,'df7_p_feav3')\n",
    "save(df14_p,'df14_p_feav3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
